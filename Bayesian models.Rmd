---
title: "Bayesian modelling"
author: "Paul V"
date: "27/01/2021"
output: html_document
---

## Libraries
```{r libraries, message = FALSE, warnings = FALSE, results = FALSE}

options(scipen = 4)
library(data.table)
library(psych)
library(standardize)
library(GPArotation)
library(lubridate)
library(vroom)
library(fasttime)
library(sjPlot)
library(naniar)
library(nFactors)
library(TTR)
library(tidyquant)
library(hrbrthemes)
library(brms)
library(R.filesets)
library(lme4)
library(bayesplot)
library(MASS)
library(minqa)
library(lmerTest)
library(GLMMadaptive)
library(ordinal)
library(tidyverse)

# For new DELL
setwd("C:/Users/paulv/Documents/R/thermostat")

```

# Data set-up (from dissertation analysis.. additions are in IMPORT/CREATE DATA-FRAME section below)
```{r DataWrangle, eval=FALSE, echo = FALSE}

#Load Values and Environmental Behaviours (VEB) Questionnaire
VEB <- fread ("VEB.csv")
as_tibble (VEB)


#Create filter for all completed and non-preview surveys: cs ('completed surveys')
cs <- VEB %>% filter(Status == "IP Address", Progress == "100")
  
#Replace Likert strings with numbers as characters
cs <- cs %>% lapply(gsub, pattern = "Strongly agree", replacement = 3)
cs <- cs %>% lapply(gsub, pattern = "Agree", replacement = 2)
cs <- cs %>% lapply(gsub, pattern = "Somewhat agree", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "Neither agree nor disagree", replacement = 0)
cs <- cs %>% lapply(gsub, pattern = "Somewhat disagree", replacement = -1)
cs <- cs %>% lapply(gsub, pattern = "Disagree", replacement = -2)
cs <- cs %>% lapply(gsub, pattern = "Strongly disagree", replacement = -3)
cs <- cs %>% lapply(gsub, pattern = "7 = Very likely", replacement = 7)
cs <- cs %>% lapply(gsub, pattern = "4 = Neither likely nor unlikely", replacement = 4)
cs <- cs %>% lapply(gsub, pattern = "1 = Not at all likely", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "5 = Very much", replacement = 5)
cs <- cs %>% lapply(gsub, pattern = "1 = Not at all", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "1 = Very conservative", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "7 = Very liberal", replacement = 7)

#Convert Crescent & Warneford to characters
cs$Q3 <- as.character(cs$Q3)

#Convert Age data to numeric
cs$Q34 <- as.numeric(cs$Q34)

#Coerce list back to tibble
cs <- as_tibble(cs)

#Rename halls to match with thermostat data
cs[cs$Q3 == "Crescent Hall", "Q3"] <- as.character("CRESCENT")
cs[cs$Q3 == "Warneford Hall", "Q3"] <- as.character("WARNEFORD")
#Combine columns to produce matchable room string
cs$site_room <- paste(cs$Q3, cs$Q4, cs$Q21)
#Remove spaces for matching
cs$site_room <- gsub('\\s+', '', cs$site_room)
#Convert to factor
cs$site_room <- as_factor(cs$site_room)

#Convert char strings to numerics for col 15-111 (survey main body)
cs [,15:111] <- as_tibble(
  sapply(cs[,15:111], as.numeric, USE.NAMES = FALSE))
cs [,115] <- as_tibble(
  sapply(cs[,115], as.numeric, USE.NAMES = FALSE))

#Define function to reverse scored items
reverse5 <- function (x) recode(x,'5' = 1, '4' = 2, `2` = 4, '1'=5) 
reverse7 <- function (x) recode(x, '3'=-3, `2` = -2, `1` = -1, `-1` = 1, `-2` = 2, '-3'=3) 

#Apply reverse scoring to NEP
cs[c(16, 18, 20, 22, 24, 26, 28)] <- lapply (
  cs[c(16, 18, 20, 22, 24, 26, 28)], reverse7)

#Apply reverse scoring to EAI
#Original: cs[c(31,33,35,37,38,41,42,44,47,49,51,53)] <- lapply (cs[c(31,33,35,37,38,41,42,44,47,49,51,53)], reverse7)
cs[c(31,33,35,36,39,41,43,44,46,48,51,53)] <- lapply (
  cs[c(31,33,35,36,39,41,43,44,46,48,51,53)], reverse7)

#Apply reverse scoring to BSCS
cs[c(100,101,102,103,105,107,108,110,111)] <- lapply (
  cs[c(100,101,102,103,105,107,108,110,111)], reverse5)

#Compute EAI mean
cs <- cs %>% rowwise() %>% mutate(EAI_mean = mean(c(
  Q6_1, Q6_2, Q6_3, Q6_4, Q6_5, Q6_7, Q6_8, Q6_9, Q6_10, Q6_11, Q6_12, Q6_13,
  Q6_14, Q6_15, Q6_16, Q6_17, Q6_18, Q6_19, Q6_20, Q6_21, Q6_22, Q6_23, Q6_24), na.rm = TRUE)) %>% ungroup()

#Create scales
NEP <- cs %>% select(Q5_1:Q5_15)
EAI <- cs %>% select(Q6_1:Q6_24)
BSCS <- cs %>% select(Q11_1:Q11_13)
likelyPEB <- cs %>% select(Q7_1:Q7_22)
thermo_moral <- cs %>% select(Q8_7:Q8_9)
moral_ascoop <- cs %>% select(Q9_1:Q9_7)
moral_found <- cs %>% select(Q10_1:Q10_7)

#Create Activist and Pragmatist variables from likely PEB scale
cs <- cs %>% rowwise() %>% mutate(
  PEB_activist = mean(c(Q7_2, Q7_3, Q7_4, Q7_5, Q7_6, Q7_8, Q7_9, Q7_10, Q7_11), na.rm = TRUE))
cs <- cs %>% rowwise() %>% mutate(
  PEB_pragmatist = mean(c(
    Q7_12, Q7_13, Q7_14, Q7_15, Q7_16, Q7_17, Q7_18, Q7_19, Q7_20, Q7_21, Q7_22), na.rm = TRUE))

#Calculate overall scores for NEP, BSCS, likelyPEB
#creates vector of means..
cs <- cbind(cs, NEP_mean = rowMeans(NEP, na.rm = TRUE))
cs <- cbind(cs, likelyPEB_mean = rowMeans(likelyPEB, na.rm = TRUE))
cs <- cbind(cs, BSCS_mean = rowMeans(BSCS, na.rm = TRUE))
cs <- cbind(cs, thermo_moral_mean = rowMeans(thermo_moral, na.rm = TRUE))
cs <- cbind(cs, MAC_mean = rowMeans(moral_ascoop, na.rm = TRUE))
cs <- cbind(cs, MFT_mean = rowMeans(moral_found, na.rm = TRUE))

#Rename column names to descriptives: cs %>% rename(new = old)
cs <- cs %>% rename('Political orientation' = Q35_1)
cs <- cs %>% rename('Habit' = Q8_2)
cs <- cs %>% rename('SocialNorm' = Q8_3)
cs <- cs %>% rename('PBC' = Q8_5)
cs <- cs %>% rename('AwarenessConsequences' = Q8_1)
cs <- cs %>% rename('Intention' = Q8_6)
cs <- cs %>% rename('AscriptionResponsibility' = Q8_4)

#Replace 0's with NA in Age
cs$Q34[cs$Q34 == 0] <- NA

#Centre Age
#cs$Q34 <- cs$Q34 - mean(cs$Q34, na.rm = TRUE)

#LOAD IN THE THERMOSTAT DATA

#Read in the Irus data (hourly, without interactions)
irus_data <- vroom("IrusData - Energy data Crescent and Warneford.csv",
  col_names = TRUE, col_select = c(1:5, 7, 10, 11, 25, 26))

#Filter for student room heaters only (takes out water heaters, kitchens, offices etc)
irus_data <- irus_data %>%
  filter(Type == "Room Heater", Name != "Office", nchar(Name) <= 6) %>%
  arrange(Name)

irus_data$Site <- as.factor(irus_data$Site)

#Load external temperature data (source: Weather Online)
external_temp <- tibble(fread("Oxford temp data.csv") %>%
  rename(Ext_temp_celsius = `Temp 2m [C]`) %>%
  select(Date, Ext_temp_celsius))
external_temp$Date <- as.Date(external_temp$Date, "%d/%m/%Y")

irus_data <- left_join(irus_data, external_temp, by = "Date")

#Merge the Site and Room Name fields
irus_data$site_room <- paste(irus_data$Site, irus_data$Name)

#Remove any spaces (to ensure matching)
irus_data$site_room <- gsub('\\s+', '', irus_data$site_room)

#Merge the date and time fields
irus_data$date_time <- fastPOSIXct(paste(irus_data$Date, irus_data$Time))

#Convert rooms to factors (for graphing)
irus_data$site_room <- as.factor(irus_data$site_room)

#Add daily mean setpoint per room and daily average per room into dataframe

irus_data <- irus_data %>% group_by(site_room, Date) %>% 
  mutate(daily_mean_sp = mean(Setpoint, na.rm = TRUE),
         daily_airtemp = mean(`Temp Air`, na.rm = TRUE))

irus_data <- left_join(irus_data, irus_data %>% 
                         group_by(site_room, Date) %>% 
                         filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
                         filter(Setpoint != 19) %>% mutate(daily_mean_sp_excdflt = mean(
                           Setpoint, na.rm = TRUE)))

irus_data <- left_join(irus_data, irus_data %>% 
                         group_by(site_room) %>% 
                         filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
                         filter(Setpoint != 19) %>% mutate(mean_sp_excdflt = mean(
                           Setpoint, na.rm = TRUE)))

#HYPOTHESIS 4 - add Before & After tags

irus_data$intervention <- NA
attach(irus_data)
irus_data$intervention[Date < as.Date("2020-02-14")] <- "Before"
irus_data$intervention[Date > as.Date("2020-02-14")] <- "After"
detach(irus_data)

#Add EXCLUDE filter for peaks
exclude <- irus_data %>% filter((Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) |
(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01"))) %>%
  group_by(date_time ,site_room ) %>%
  summarise(temp = Setpoint) %>%  summarise(temp = mean(temp, na.rm = TRUE)) %>% 
  arrange(desc(temp)) %>% filter (temp > 20) %>% pull(date_time)

# Add EXCLUDE flag to Irus dataframe - 0 = ones to ignore; 1 = ones to keep
irus_data <- irus_data %>% ungroup() %>% mutate(exclude = (ifelse(irus_data$date_time %in% exclude, 0, 1)))

#Remove all EXCLUDE = 0 from irus_data
irus_data <- irus_data %>% filter(exclude == 1)

#Calc mean room temp before and after posters (14th Feb) for each room & append to irus_data
irus_data <- left_join (irus_data, irus_data %>% 
  filter (Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% 
  group_by(site_room) %>% 
  summarise(avg_setpoint_before = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
   group_by(site_room) %>%
   summarise(avg_setpoint_after = mean(Setpoint)), by = "site_room")

#Calculate with cleaned data - exclude peaks
irus_data <- left_join (irus_data, irus_data %>% 
  filter (Date > as.Date("2020-01-30") & Date < as.Date(  "2020-02-14")) %>% filter (exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_setpoint_beforeCL = (mean(Setpoint) - 19)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% filter (exclude == 1) %>%
   group_by(site_room) %>%
   summarise(avg_setpoint_afterCL = (mean(Setpoint) - 19)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-03-01")) %>% filter (exclude == 1) %>%
   group_by(site_room) %>%
   summarise(avg_setpointCL = (mean(Setpoint) - 19)), by = "site_room")

#Add Setpoint exc. default as new variable
Setpoint_excl_default <- irus_data %>% filter(Setpoint != 19) %>% select(Setpoint, site_room, date_time)
irus_data <- left_join(irus_data, Setpoint_excl_default, by = c("site_room", "date_time"))
names(irus_data)[names(irus_data) == 'Setpoint.y'] <- 'Setpoint_excl_default'
names(irus_data)[names(irus_data) == 'Setpoint.x'] <- 'Setpoint'
 
#Calculate before and after average thermostat set points EXC. default settings + add to irus_data
irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdflt = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% 
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdflt = mean(Setpoint)), by = "site_room")


#Add EXC defaults, EXC peaks data to Irus
irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdfltCL = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% 
  filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

#MEANS
irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)) %>% as.data.frame()

irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% filter(exclude == 1) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)) %>% as.data.frame()

irus_data <- irus_data %>% mutate (thermo_change_excdflt = avg_sp_after_excdflt - avg_sp_before_excdflt)

#Calculate number of settings below 19 degrees, before and after 14th Feb (posters date)

#Add thermostat data inc defaults to CS
cs <- left_join(cs, irus_data %>% 
    filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% 
    group_by(site_room) %>% 
    summarise(avg_setpoint_before = mean(Setpoint)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-02-14") & Date < as.Date(  "2020-03-01")) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpoint_after = mean(Setpoint)), by = "site_room")


cs <- left_join(cs, irus_data %>% 
    filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>%
    group_by(site_room) %>% 
    summarise(avg_setpoint_beforeCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-02-14") & Date < as.Date(  "2020-03-01")) %>% filter(exclude == 1) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpoint_afterCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-01-30") & Date < as.Date(  "2020-03-01")) %>% filter(exclude == 1) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpointCL = (mean(Setpoint)-19)), by = "site_room")


#Add thermostat data exc defaults

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdflt = mean(Setpoint)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdflt = mean(Setpoint)), by = "site_room")

cs <- cs %>% mutate(avg_sp_before_excdflt19 = avg_sp_before_excdflt - 19,
                    avg_sp_after_excdflt19 = avg_sp_after_excdflt - 19)

#Add data exc dflt & exc peaks (cleaned)
cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
  filter(exclude ==1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-03-01")) %>%
  filter(exclude ==1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_overall_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

#Add change variable
cs <- cs %>% mutate(thermo_changeCL = avg_setpoint_afterCL - avg_setpoint_beforeCL)
cs <- cs %>% mutate (thermo_change_excdfltCL = avg_sp_after_excdfltCL - avg_sp_before_excdfltCL)

#Standardise key variables
cs <- cs %>% mutate(likelyPEB_mean_sz = as.numeric(scale(likelyPEB_mean)))
cs <- cs %>% mutate(EAI_mean_sz = as.numeric(scale(EAI_mean)))
cs <- cs %>% mutate(NEP_mean_sz = as.numeric(scale(NEP_mean)))
cs <- cs %>% mutate(PEB_activist_sz = as.numeric(scale(PEB_activist)))
cs <- cs %>% mutate(PEB_pragmatist_sz = as.numeric(scale(PEB_pragmatist)))
cs <- cs %>% mutate(thermo_moral_mean_sz = as.numeric(scale(thermo_moral_mean)))
cs <- cs %>% mutate(MAC_mean_sz = as.numeric(scale(MAC_mean)))
cs <- cs %>% mutate(MFT_mean_sz = as.numeric(scale(MFT_mean)))
# cs <- cs %>% mutate(sub19_before_sz = as.numeric(scale(sub19_before)))
# cs <- cs %>% mutate(sub19_after_sz = as.numeric(scale(sub19_after)))
cs <- cs %>% mutate(avg_setpoint_before_sz = as.numeric(scale(avg_setpoint_before)))
cs <- cs %>% mutate(avg_setpoint_after_sz = as.numeric(scale(avg_setpoint_after)))
cs <- cs %>% mutate(avg_sp_before_excdflt_sz = as.numeric(scale(avg_sp_before_excdflt)))
cs <- cs %>% mutate(avg_sp_after_excdflt_sz = as.numeric(scale(avg_sp_after_excdflt)))
cs <- cs %>% mutate(thermo_changeCL_sz = as.numeric(scale(thermo_changeCL)))
# cs <- cs %>% mutate(sub19_change_sz = as.numeric(scale(sub19_change)))
cs <- cs %>% mutate(thermo_change_excdfltCL_sz = as.numeric(scale(thermo_change_excdfltCL)))
cs <- cs %>% mutate(AwarenessConsequences_sz = as.numeric(scale(AwarenessConsequences)))
cs <- cs %>% mutate(Habit_sz = as.numeric(scale(Habit)))
cs <- cs %>% mutate(Intention_sz = as.numeric(scale(Intention)))
cs <- cs %>% mutate(SocialNorm_sz = as.numeric(scale(SocialNorm)))
cs <- cs %>% mutate(AscriptionResponsibility_sz = as.numeric(scale(AscriptionResponsibility)))
cs <- cs %>% mutate(PBC_sz = as.numeric(scale(PBC)))
cs <- cs %>% mutate(BSCS_mean_sz = as.numeric(scale(BSCS_mean)))
cs <- cs %>% mutate(PoliticalOrientation_sz = as.numeric(scale(`Political orientation`)))




```

# EXPORT FILES FOR QUICK ACCESS (if required)
```{r FileExport, eval = FALSE, echo=FALSE}

# main data file for longitudinal modelling (LARGE; 700k + rows)
write_csv(irus_data, file = "IrusData.csv")

# data file for survey data (Small; 88 rows)
write_csv(cs, file = "SurveyData.csv")
```

# IMPORT dissertation dataframe AND CREATE new DATA-FRAME (df) for modelling
```{r DataImport, echo=FALSE, results = FALSE, eval=FALSE}

#Import csv's
irus_data <- read.csv("IrusData.csv")
cs <- read.csv("SurveyData.csv")

#Convert date_time variable to correct to POSIXt format
irus_data$date_time <- as_datetime(irus_data$date_time)

#Create new data-frame for temperature modelling with appropriate variables
df <- irus_data %>%
  filter(Date >= as.Date("2020-02-01")) %>%  #dropping all data points before Feb 1
  filter(Date <= as.Date("2020-02-29")) %>%
  filter(Date != as.Date("2020-02-14")) %>% #remove 14th Feb as intervention time unknown
  filter(is.na(Setpoint) == 0) %>%
  #filter(Setpoint != 19) %>% #exclude all default values
  mutate(time_elapsed = as.numeric((date_time) - date_time[1])/3600) %>% #zero time clock and convert to hours
  mutate(temp.z = (Setpoint - mean(Setpoint,  na.rm=T))/sd(Setpoint, na.rm=T)) %>% #standardise set-point temperature
  mutate(temp.c = Setpoint - 19) %>% #create centred (non-standardised) temp setpoint measure = difference from default
  mutate(external.z = (Ext_temp_celsius - mean(Ext_temp_celsius,  na.rm=T))/sd(Ext_temp_celsius, na.rm=T)) #standardise external temperature

#Replace all unique room names (characters) with number for modelling
cross_table <- data.frame(Name = unique(df$Name),Code = seq(1:length(unique(df$Name))))
df$Address <- as.integer(lapply(df$Name, function(x) cross_table$Code[match(x, cross_table$Name)]))

# Convert intervention variable into factor
df$intervention <- factor(df$intervention, levels = c("Before", "After"))

#Take sample of data to test models (faster?)
#df_sample <- df[sample(nrow(df), 50000), ]

#Dataframe excluding temp = 19 default values
df_xd <- df %>% filter(Setpoint != 19)

#rescale time_elapsed variable - that works!
df$time_elapsed_r <- df$time_elapsed/300
df_xd$time_elapsed_r <- df_xd$time_elapsed/300

#create moralisation score
#create new variable = moralisation from Q8_7 and Q8_9
cs <- cs %>% rowwise() %>% mutate(moralisation = mean(c(Q8_7, Q8_9)))
#standardise
cs$moralisation.z <- (cs$moralisation - mean(cs$moralisation))/sd(cs$moralisation)

#Count instances of room numbers - there is one duplicate in Crescent - Room L04F
data.frame(table(cs$site_room))

#remove duplicate room - CRESCENTL04F
cs <- cs %>% filter(site_room != "CRESCENTL04F")

#Create survey data data-frame by matching all completed surveys (cs) variables to main df only for those that have completed

df_survey <- inner_join(df, cs[,c("site_room", "Habit_sz", "PBC_sz", "moralisation.z",
                                   "AscriptionResponsibility_sz", "SocialNorm_sz",
                                   "AwarenessConsequences_sz", "PEB_pragmatist_sz",
                                   "PEB_activist_sz", "NEP_mean_sz", "Intention_sz",
                                  "Political.orientation", 'Q34', 'Q17', 'Q13')],
                        by = "site_room")
df_survey_xd <- inner_join(df_xd, cs[,c("site_room", "Habit_sz", "PBC_sz", "moralisation.z",
                                   "AscriptionResponsibility_sz", "SocialNorm_sz",
                                   "AwarenessConsequences_sz", "PEB_pragmatist_sz",
                                   "PEB_activist_sz", "NEP_mean_sz", "Intention_sz",
                                  "Political.orientation", 'Q34', 'Q17', 'Q13')],
                           by = "site_room")

df_survey <- rename(df_survey, 'Age' = 'Q34', 'Born_UK' = 'Q17', 'Gender' = 'Q13')


#Rename site locations with communications condition
df$Site <- recode_factor(df$Site, CRESCENT = "FAMILY", WARNEFORD = "DIRECT")
df_xd$Site <- recode_factor(df_xd$Site, CRESCENT = "FAMILY", WARNEFORD = "DIRECT")

#generate sample of 50 rooms for smaller dataframe for model testing

df_sample <- filter(df, Address %in%  sample(1:551, 50))

#save as rds objects for easy recovery
saveRDS(df_survey, "df_survey.rds")
saveRDS(df_survey_xd, "df_survey_xd.rds")
saveRDS(df, "df.rds")
saveRDS(df_xd, "df_xd.rds")
saveRDS(cs, "cs.rds")
saveRDS(df_sample, "df_sample.rds")
saveRDS(irus_data, "irus_data.rds")


```

# New dataframe load in RDS objects
```{r}

df_survey <- loadRDS("df_survey.rds")
df_survey_xd <- loadRDS("df_survey_xd.rds")
df <- loadRDS("df.rds")
df_xd <- loadRDS("df_xd.rds")
cs <- loadRDS("cs.rds")
df_sample <- loadRDS("df_sample.rds")
irus_data <- loadRDS("irus_data.rds")


```

##Visualisation
```{r Visualisation, eval=FALSE, warning=FALSE}

p <- irus_data %>% 
  group_by(Date) %>% filter((Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) |
                              (Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01"))) %>%
  summarise(External = mean(Ext_temp_celsius), 
            `Thermostat setting exc. defaults` = mean(daily_mean_sp_excdflt, na.rm = TRUE),
            `Thermostat setting` = mean(Setpoint, na.rm = TRUE),
            sd = sd(Setpoint_excl_default, na.rm = TRUE), sd1 = sd(Setpoint, na.rm = TRUE)) %>% 
  mutate(lower = `Thermostat setting exc. defaults` - sd, upper = `Thermostat setting exc. defaults` + sd) %>% 
  mutate(lower1 = `Thermostat setting` - sd1, upper1 = `Thermostat setting` + sd1) %>%
  mutate(Date = as.Date(Date))

p %>%  ggplot() +
  geom_line(aes(x = Date, y = `Thermostat setting exc. defaults`, colour = "Thermostat setting exc. defaults", 
                group = 1)) +
  geom_errorbar(aes(x = Date, ymin = lower, ymax = upper), lty = "longdash", 
                width = 0.5, colour = "red", alpha = 0.3) +
  geom_errorbar(aes(x = Date, ymin = lower1, ymax = upper1), lty = "longdash", 
                width = 0.2, colour = "black", alpha = 0.3) +
  geom_line(aes(x = Date, y = External, colour = "External", group = 1)) +
  labs(y = "Temp (celsius)", caption = "Notes: External temperature sourced from WeatherOnline.co.uk; Thermostat data is shown with and without\nthe system default value of 19 degrees; Error bars show 1 standard deviation") +
  geom_line(aes(x = Date, y = `Thermostat setting`, colour = "Thermostat setting"), linetype = "dotted", group = 1) +
  scale_color_manual(values = c(
    "External" = "royalblue3", "Thermostat setting" = "black", "Thermostat setting exc. defaults" = "red")) +
  scale_x_date(date_labels = "%b %d") +
  scale_y_continuous(breaks = c(0:25)) +
  theme(legend.title = element_blank(), legend.position="bottom", plot.caption = element_text(hjust = 0)) 

ggsave("temp plot.png", device = "png")

#Correlation between external temp and thermostat setting
cor.test(irus_data$daily_mean_sp_excdflt, irus_data$Ext_temp_celsius)

#Do the whole thing as one big correlation matrix - ACTUAL behaviour
#M is only correlation matrix, M_full (from corr.test) includes p-values and conf. intervals

M <- cs %>% select(avg_sp_overall_excdfltCL, PEB_activist, PEB_pragmatist, 'AwarenessConsequences', 'AscriptionResponsibility', 
              NEP_mean, 'SocialNorm', 'PBC', 'Habit', 'Intention', moralisation.z) %>% cor(use = "pairwise.complete.obs")

colnames(M) <- c("Mean setpoint (exc defaults)",
                 "PEB Activist",
                 "PEB Pragmatist",
                 "Awareness of consequences",
                 "Acsription of responsibility",
                 "NEP",
                 "Social norm",
                 "Perceived behavioural control",
                 "Habit",
                 "Intention",
                 "Moralisation")

rownames(M) <- c("Mean setpoint (exc defaults)",
                 "PEB Activist",
                 "PEB Pragmatist",
                 "Awareness of consequences",
                 "Ascription of responsibility",
                 "NEP",
                 "Social norm",
                 "Perceived behavioural control",
                 "Habit",
                 "Intention",
                 "Moralisation")

#Use corr.p function to get p values r values and confidence intervals from correlation matrix M
res1 <- corr.p(M, 86) # Note: 86 observations for thermo_exc_dfltCL as 2 taken out as duplicated (Room L04F)

#Write CI's to text file
write.table(res1$ci, file = "corr_actuals.txt", sep = ",", quote = FALSE, row.names = T)

#Remove all non-significant (p > .05) correlations
M[res1$p > .05] <- NA


#New plot
corrplot::corrplot(M, col = 'gray', number.cex =  .8, tl.col="black", addCoef.col = 'black', type = "lower", diag = FALSE, cl.pos = 'n', na.label=" ", method = "square")

#Note there are no major differences between demographic make up of the sites
describe(df_survey %>% filter(Site == "CRESCENT") %>% select(Political.orientation, Age, Born_UK, Gender))
describe(df_survey %>% filter(Site == "WARNEFORD") %>% select(Political.orientation, Age, Born_UK, Gender))

#Modelling manipulation effect - linear models
lm1 <- lm(temp.c ~ intervention, data = df)
summary(lm1)
lm2 <- lm(temp.c ~ time_elapsed_r + intervention, data = df)
summary(lm2)
lm3 <- lm(temp.c ~ time_elapsed_r + intervention + external.z, data = df)
summary(lm3)


# histogram of temp distributions - full data frame and excluding defaults 

options(scipen = 6) # sets threshold for number of characters to 6 for scientific notation

h1 <- df %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13)
h2 <- df_xd %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13)
h3 <- df %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13) + facet_wrap(df$intervention) + ggtitle("Data including default settings (19 celsius)") + theme(plot.title = element_text(size = 10))
h4 <- df_xd %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13) + facet_wrap(df_xd$intervention)+ ggtitle("Data excluding default settings") + theme(plot.title = element_text(size = 10))

gridExtra::grid.arrange(h3, h4)

df %>%
  ggplot( aes(x=temp.c, fill=intervention), binwidth = .5) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'dodge') +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")



h5 <- df_xd %>% ggplot(aes(x=temp.c, fill=intervention)) + geom_histogram(binwidth = .5, color="#e9ecef", alpha=0.6, position = 'dodge') + xlim(-5,5) + facet_wrap(~ Site) + ggtitle("Data excluding default settings") +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum(plot_title_size = 9, plot_title_face = "plain", strip_text_size = 8)
h5

#Density plots


d1 <- ggplot(data=df, aes(x=temp.c, group=intervention, fill=intervention)) +
    geom_density(adjust=1.5, alpha=.3) + xlim(-5,5) + labs(x = "Temperature difference from default") + theme(axis.title.y = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + scale_fill_discrete(name = "Intervention") + ggtitle("INCLUDING default settings") +     theme(plot.title = element_text(size = 10))


d2 <- ggplot(data=df_xd, aes(x=temp.c, group=intervention, fill=intervention)) +
    geom_density(adjust=1.5, alpha=.3) + xlim(-5,5) + labs(x = "Temperature difference from default") + theme(axis.title.y = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black")) + scale_fill_discrete(name = "Intervention") + ggtitle("EXCLUDING default settings") +     theme(plot.title = element_text(size = 10))

d3 <- gridExtra::grid.arrange(d1, d2)

ggsave("density plot.png", plot = d3, device = "png")

d4 <- df_xd %>% ggplot(aes(x=temp.c, fill=intervention)) + geom_density(adjust=1.5, alpha=.3) + xlim(-5,5) + facet_wrap(~ Site) + scale_fill_discrete(name = "Intervention") +
    theme_ipsum(plot_title_size = 9, plot_title_face = "plain", strip_text_size = 8) + labs(x = "Temperature difference from default") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.title.y = element_blank(), axis.title.x = element_text(hjust = 0.5),
axis.line = element_line(colour = "black"))

ggsave("density plot2.png", plot = d4, device = "png", width = 7, height = 4)

```

## Mixed models
```{r MixedModels, eval=FALSE}

# WITH rescaled time axis, the models converge..

#Including defaults
mm1 <- lmer(temp.c ~ (time_elapsed_r|Address), data = df)
summary(mm1)

#Excluding defaults
mmxd1 <- lmer(temp.c ~ (time_elapsed_r|Address), data = df_xd)
summary(mmxd1)

# No significant effect of time as a fixed effect (t value < 1)
mm2 <- lmer(temp.c ~ time_elapsed_r + (time_elapsed_r|Address), data = df)
summary(mm2)

# Controlling for external temp
# Significant effect of external temp. - slightly greater effect of time 
mm3 <- lmer(temp.c ~ time_elapsed_r + Ext_temp_celsius + (time_elapsed_r|Address), data = df)
summary(mm3)

#Then ext temp and intervention together
# Shows intervention has an effect too
mm5 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address), data = df, control = lmerControl(optimizer = "bobyqa"))
summary(mm5)

# equivalent model excluding defaults
mmx5 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address), data = df_xd)
summary(mmx5)

confint.merMod(mmx5)

#Check for interaction effect of Site*Intervention
#Shows that the intervention effect is driven largely (wholly??) by WARNEFORD site
# Note: from memory we found this before - there was a big effect in one hall and not the other
# personally I doubt this was solely due to a few words different on the poster, but that's for discussion
mm6 <- lmer(temp.c ~ time_elapsed_r + intervention*Site + Ext_temp_celsius + (time_elapsed_r|Address), data = df)
summary(mm6)

mmx6 <- lmer(temp.c ~ time_elapsed_r + intervention*Site + Ext_temp_celsius + (time_elapsed_r|Address), data = df_xd)
summary(mmx6)

tab_model(mmx5, mmx6)


#Random effects of time, within room and site implicitly nested 
mm7 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address) + (time_elapsed_r|Site), data = df)
summary(mm7)


```


## Ordinal models for multi-nomial (categorical) DV
```{r OrdinalModels, eval=TRUE, echo=FALSE}

#Add outcome variable to dataframe
df$behaviour <- NA
df$behaviour[df$Setpoint == 19] = "default"
df$behaviour[df$Setpoint < 19] = "lower"
df$behaviour[df$Setpoint > 19] = "higher"
df$behaviour <- factor(df$behaviour, ordered = TRUE, levels = c("lower", "default", "higher"))

# Ordinal dependent variable models from https://rpubs.com/rslbliss/r_logistic_ws

# Model 1
model_OD1 <- polr(behaviour ~ time_elapsed_r + intervention*Site + Ext_temp_celsius, data = df, Hess=TRUE)
summary(model_OD1)

## calculate and store p values
ctable <- coef(summary(model_OD1))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
## combined table
(ctable <- cbind(ctable, "p value" = p))

## odds ratios and confidence intervals
ci <- confint.default(model_OD1)
exp(cbind("Odds ratio" = coef(model_OD1), ci))

#convert Address to factor
df$Address <- factor(df$Address)
#same for Site
df$Site <- factor(df$Site)

#Model 2 pro-environmental behaviour as 1 otherwise 0 (logisitic regression) with random effect
df$behaviour <- 0
df$behaviour[df$Setpoint < 19] <- 1
m2 <- glmer(behaviour ~ time_elapsed_r + intervention*Site + Ext_temp_celsius + (1|site_room), data = df, family = "binomial", nAGQ = 0, control = glmerControl(optimizer="bobyqa"))
summary(m2)

sum(df$behaviour == 0 & df$intervention == "Before")
sum(df$behaviour == 0 & df$intervention == "After")
sum(df$behaviour == 1 & df$intervention == "Before")
sum(df$behaviour == 1 & df$intervention == "After")

sum(df$behaviour == 1 & df$intervention == "Before")/(sum(df$behaviour == 0 & df$intervention == "Before")+sum(df$behaviour == 1 & df$intervention == "Before"))

sum(df$behaviour == 1 & df$intervention == "After")/(sum(df$behaviour == 0 & df$intervention == "After")+sum(df$behaviour == 1 & df$intervention == "After"))

```

## Survey data
```{r SurveyDataMixedModels, eval=FALSE}


####Manipulation effect in sub-sample with questionnaire data
#NO EXCLUSIONS
m1 <- lmer(temp.c ~ 1 + time_elapsed_r + (time_elapsed_r|site_room), data=df_survey)
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m3 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m4 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m5 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
tab_model(m1, m2, m3, m4, m5)

#EXCLUding DEFAULT DATA
m1 <- lmer(temp.c ~ 1 + time_elapsed_r + (time_elapsed_r|site_room), data=df_survey_xd)
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m3 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m4 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + Site + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m5 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
tab_model(m1, m2, m3, m4, m5)



# Variables seem to have no effect 

#NEP
m1 <- lmer(temp.c ~ 1 + NEP_mean_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

# Pro-environmental behaviours - ACTIVIST
m1 <- lmer(temp.c ~ 1 + PEB_activist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

# Pro-environmental behaviours - PRAGMATIST
m1 <- lmer(temp.c ~ 1 + PEB_pragmatist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

#HABIT
m1 <- lmer(temp.c ~ 1 + Habit_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

#SOCIAL NORM
m1 <- lmer(temp.c ~ 1 + SocialNorm_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)


#These variables are all p < .05 - strongest are intention and moralisation

#AWARENESS OF CONSEQUENCES
m1 <- lmer(temp.c ~ 1 + AwarenessConsequences_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

#ASCRIPTION OF RESPONSIBILITY
m2 <- lmer(temp.c ~ 1 + AscriptionResponsibility_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m2)

#PERCEIVED BEHAVIOURAL CONTROL
m3 <- lmer(temp.c ~ 1 + PBC_sz + time_elapsed_r +external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m3)

#MORALISATION
m4 <- lmer(temp.c ~ 1 + moralisation.z + time_elapsed_r +external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m4)

#BEHAVIOURAL INTENTION
m5 <- lmer(temp.c ~ 1 + Intention_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m5)
tab_model(m1, m2, m3, m4, m5)

#Check Activist v Pragmatist intention
m1 <- lmer(temp.c ~ 1 + PEB_pragmatist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m2 <- lmer(temp.c ~ 1 + PEB_activist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m2)
tab_model(m1, m2, m5)


# Find EFFECT SIZE size of moralisation on exc default data
m1 <- lmer(temp.c ~ 1 + moralisation.z + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey_xd)
summary(m1)
confint(m1)

```



# Moderation / Mediation effects 
```{r Med/Mod Effects, eval=FALSE}

#For moderation convert intervention to dummy variable from factor
df_survey$intervention <- as.numeric(df_survey$intervention)

#Fit basic models + psych variables 3-way interaction (note: standardised temp)

#Intention
m_int <- lmer(temp.z ~ 1 + time_elapsed_r + external.z + intervention * Intention_sz + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
#Ascription responsibility
m_asc <- lmer(temp.z ~ 1 + time_elapsed_r + external.z + intervention * AscriptionResponsibility_sz + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
#Perceived Behavioural Control
m_pbc <- lmer(temp.z ~ 1 + time_elapsed_r + external.z + intervention * PBC_sz + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
#Moralisation
m_mrl <- lmer(temp.z ~ 1 + time_elapsed_r + external.z + intervention * moralisation.z + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

#MODERATION

#INTENTION: Shows that high intention people actually increase temp post-intervention 
interactions::probe_interaction(m_int,pred=intervention,modx='Intention_sz')

#ASCRIPTION: 
interactions::probe_interaction(m_asc,pred=intervention,modx='AscriptionResponsibility_sz')

#PBC: 
interactions::probe_interaction(m_pbc,pred=intervention,modx='PBC_sz')

#MORALISATION: 
interactions::probe_interaction(m_mrl,pred=intervention,modx='moralisation.z')

#In all cases the overall temperature settings drops post intervention, but drops more amongst those who have LOWER scores on the key variables

# DV = temp setting; IV = intervention; mediator = intention
med1 <- psych::mediate('temp.c', 'intervention', m = c('Intention_sz'), data = df_survey)
med1
mediate.diagram(med1, main = "c = Direct Effect; c' = Indirect Effect", xlim = c(0,10), ylim = c(2,9), l.cex = .7)

#Check the relationship between intervention and Intention
lm(Intention_sz ~ intervention, data = df_survey)

# DV = temp setting; IV = intervention; mediator = Ascription
med2 <- psych::mediate('temp.c', 'intervention', m = c('AscriptionResponsibility_sz'), data = df_survey)
med2
mediate.diagram(med2, main = "c = Direct Effect; c' = Indirect Effect", xlim = c(0,10), ylim = c(2,9), l.cex = .7)

#Check the relationship between intervention and Ascription
lm(AscriptionResponsibility_sz ~ intervention, data = df_survey)

# DV = temp setting; IV = intervention; mediator = PBC
med3 <- psych::mediate('temp.c', 'intervention', m = c('PBC_sz'), data = df_survey)
med3
mediate.diagram(med3, main = "c = Direct Effect; c' = Indirect Effect", xlim = c(0,10), ylim = c(2,9), l.cex = .7)

#Check the relationship between intervention and PBC
lm(PBC_sz ~ intervention, data = df_survey)

# DV = temp setting; IV = intervention; mediator = moralisation
med4 <- psych::mediate('temp.c', 'intervention', m = c('moralisation.z'), data = df_survey)
med4
mediate.diagram(med4, main = "c = Direct Effect; c' = Indirect Effect", xlim = c(0,10), ylim = c(2,9), l.cex = .7)

#Check the relationship between intervention and moralisation
lm(moralisation.z ~ intervention, data = df_survey)

#Demographic variables
#political
m5 <- lmer(temp.c ~ 1 + Political.orientation + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)


#gender
#ignore single non-binary participant
m5 <- lmer(temp.c ~ 1 + Gender + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=(df_survey %>% filter(Gender != 'Different Identity')), control = lmerControl(optimizer = "bobyqa"))
summary(m5)


#UK - SIGNIFICANT
m5 <- lmer(temp.c ~ 1 + Born_UK + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)



#Age - convert to age from 18 = 0
df_survey$Age <- df_survey$Age - min(df_survey$Age)
m5 <- lmer(temp.c ~ 1 + Age + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)


#In multi-variate models only INTENTION is significant
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + Intention_sz + (time_elapsed_r|site_room),data=df_survey)
summary(m2)
tab_model(m2)

# Excluding Intention, Moralisation is still not significant
m3 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + (time_elapsed_r|site_room),data=df_survey)
summary(m3)
tab_model(m3)

# Same result on excl. default data
m4 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + (time_elapsed_r|site_room),data=df_survey_xd)
summary(m4)
tab_model(m4)

```


## Bayesian Models
```{r BayesModels, eval = FALSE, echo = FALSE}


# Fit models using sample data-frame
bms1 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address), 
            data = df_sample, file=("bms1"), file_refit = getOption("brms.file_refit", "on_change"),
            cores = 4, iter = 4000,
            prior = set_prior("normal(0,0.5)", class = "b"))

summary(bms1)

#Fit model using full data-set
bm1 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address), 
            data = df, file=("bm1"), file_refit = getOption("brms.file_refit", "on_change"),
            cores = 4, iter = 5000,
            prior = set_prior("normal(0,0.4)", class = "b"))

summary(bm1)

#need to add cores = 4 as default is 1

# NULL model

# Initial null model - intercept only - does converge
bm1 <- brm(formula = temp.c ~ (time_elapsed|Address), data = df,
           warmup = 1000, iter = 3000, cores = 4, control = list(adapt_delta = 0.95),
           file=("bm1"), file_refit = getOption("brms.file_refit", "on_change"))

#NULL model with transposed time - NOT yet FIT
bm1r <- brm(formula = temp.c ~ time_elapsed_r + (time_elapsed_r|Address), data = df,
           file=("bm1r"), file_refit = getOption("brms.file_refit", "on_change"))

#temp depends on intervention - model COMPLETE
bm2 <- brm(formula = temp.c ~ intervention,
          data = df, cores = 4,
          file=("bm2"), control = list(adapt_delta = 0.95),
          file_refit = getOption("brms.file_refit", "on_change"))

#temp depends on intervention - model DOES NOT CONVERGE (YET)
bm3 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address),
          data = df, cores = 4, warmup = 1500, iter = 4000,
          file=("bm3"), file_refit = getOption("brms.file_refit", "on_change"))

n_divergent <- function(x) {
    stopifnot(is(x, "brmsfit"))
    out <- lapply(x$fit@sim$samples, function(y) 
        sum(attr(y, "sampler_params")[["divergent__"]]))
    sum(unlist(out))
}

n_divergent(bm1)
n_divergent(bm2)
n_divergent(bm3)

#model visualisation and diagnostics
posterior1 <- as.array(bms1)

# Trace plot of Linear Regression, define variables to plot via pars flag
mcmc_trace(posterior1,  pars = c("b_Intercept", "sigma"), 
           n_warmup = 1000, facet_args = list(ncol = 2, labeller = label_parsed))

mcmc_acf(posterior1, pars = c("b_Intercept", "sigma"), 
         lags = 15)

```


