---
title: "Bayesian modelling"
author: "Paul V"
date: "27/01/2021"
output: html_document
---

## Libraries
```{r libraries, message = FALSE, warnings = FALSE}

library(data.table)
library(psych)
library(standardize)
library(GPArotation)
library(lubridate)
library(vroom)
library(fasttime)
library(sjPlot)
library(naniar)
library(nFactors)
library(TTR)
library(tidyquant)
library(hrbrthemes)
library(brms)
library(R.filesets)
library(lme4)
library(bayesplot)
library(MASS)
library(minqa)
library(lmerTest)
library(GLMMadaptive)
library(ordinal)
library(tidyverse)

# For new DELL
setwd("C:/Users/paulv/Documents/R/thermostat")

```

## Data set-up (from dissertation analysis.. additions are in IMPORT/CREATE DATA-FRAME section below)
```{r DataWrangle, eval=FALSE, echo = FALSE}

#Load Values and Environmental Behaviours (VEB) Questionnaire
VEB <- fread ("VEB.csv")
as_tibble (VEB)


#Create filter for all completed and non-preview surveys: cs ('completed surveys')
cs <- VEB %>% filter(Status == "IP Address", Progress == "100")
  
#Replace Likert strings with numbers as characters
cs <- cs %>% lapply(gsub, pattern = "Strongly agree", replacement = 3)
cs <- cs %>% lapply(gsub, pattern = "Agree", replacement = 2)
cs <- cs %>% lapply(gsub, pattern = "Somewhat agree", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "Neither agree nor disagree", replacement = 0)
cs <- cs %>% lapply(gsub, pattern = "Somewhat disagree", replacement = -1)
cs <- cs %>% lapply(gsub, pattern = "Disagree", replacement = -2)
cs <- cs %>% lapply(gsub, pattern = "Strongly disagree", replacement = -3)
cs <- cs %>% lapply(gsub, pattern = "7 = Very likely", replacement = 7)
cs <- cs %>% lapply(gsub, pattern = "4 = Neither likely nor unlikely", replacement = 4)
cs <- cs %>% lapply(gsub, pattern = "1 = Not at all likely", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "5 = Very much", replacement = 5)
cs <- cs %>% lapply(gsub, pattern = "1 = Not at all", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "1 = Very conservative", replacement = 1)
cs <- cs %>% lapply(gsub, pattern = "7 = Very liberal", replacement = 7)

#Convert Crescent & Warneford to characters
cs$Q3 <- as.character(cs$Q3)

#Convert Age data to numeric
cs$Q34 <- as.numeric(cs$Q34)

#Coerce list back to tibble
cs <- as_tibble(cs)

#Rename halls to match with thermostat data
cs[cs$Q3 == "Crescent Hall", "Q3"] <- as.character("CRESCENT")
cs[cs$Q3 == "Warneford Hall", "Q3"] <- as.character("WARNEFORD")
#Combine columns to produce matchable room string
cs$site_room <- paste(cs$Q3, cs$Q4, cs$Q21)
#Remove spaces for matching
cs$site_room <- gsub('\\s+', '', cs$site_room)
#Convert to factor
cs$site_room <- as_factor(cs$site_room)

#Convert char strings to numerics for col 15-111 (survey main body)
cs [,15:111] <- as_tibble(
  sapply(cs[,15:111], as.numeric, USE.NAMES = FALSE))
cs [,115] <- as_tibble(
  sapply(cs[,115], as.numeric, USE.NAMES = FALSE))

#Define function to reverse scored items
reverse5 <- function (x) recode(x,'5' = 1, '4' = 2, `2` = 4, '1'=5) 
reverse7 <- function (x) recode(x, '3'=-3, `2` = -2, `1` = -1, `-1` = 1, `-2` = 2, '-3'=3) 

#Apply reverse scoring to NEP
cs[c(16, 18, 20, 22, 24, 26, 28)] <- lapply (
  cs[c(16, 18, 20, 22, 24, 26, 28)], reverse7)

#Apply reverse scoring to EAI
#Original: cs[c(31,33,35,37,38,41,42,44,47,49,51,53)] <- lapply (cs[c(31,33,35,37,38,41,42,44,47,49,51,53)], reverse7)
cs[c(31,33,35,36,39,41,43,44,46,48,51,53)] <- lapply (
  cs[c(31,33,35,36,39,41,43,44,46,48,51,53)], reverse7)

#Apply reverse scoring to BSCS
cs[c(100,101,102,103,105,107,108,110,111)] <- lapply (
  cs[c(100,101,102,103,105,107,108,110,111)], reverse5)

#Compute EAI mean
cs <- cs %>% rowwise() %>% mutate(EAI_mean = mean(c(
  Q6_1, Q6_2, Q6_3, Q6_4, Q6_5, Q6_7, Q6_8, Q6_9, Q6_10, Q6_11, Q6_12, Q6_13,
  Q6_14, Q6_15, Q6_16, Q6_17, Q6_18, Q6_19, Q6_20, Q6_21, Q6_22, Q6_23, Q6_24), na.rm = TRUE)) %>% ungroup()

#Create scales
NEP <- cs %>% select(Q5_1:Q5_15)
EAI <- cs %>% select(Q6_1:Q6_24)
BSCS <- cs %>% select(Q11_1:Q11_13)
likelyPEB <- cs %>% select(Q7_1:Q7_22)
thermo_moral <- cs %>% select(Q8_7:Q8_9)
moral_ascoop <- cs %>% select(Q9_1:Q9_7)
moral_found <- cs %>% select(Q10_1:Q10_7)

#Create Activist and Pragmatist variables from likely PEB scale
cs <- cs %>% rowwise() %>% mutate(
  PEB_activist = mean(c(Q7_2, Q7_3, Q7_4, Q7_5, Q7_6, Q7_8, Q7_9, Q7_10, Q7_11), na.rm = TRUE))
cs <- cs %>% rowwise() %>% mutate(
  PEB_pragmatist = mean(c(
    Q7_12, Q7_13, Q7_14, Q7_15, Q7_16, Q7_17, Q7_18, Q7_19, Q7_20, Q7_21, Q7_22), na.rm = TRUE))

#Calculate overall scores for NEP, BSCS, likelyPEB
#creates vector of means..
cs <- cbind(cs, NEP_mean = rowMeans(NEP, na.rm = TRUE))
cs <- cbind(cs, likelyPEB_mean = rowMeans(likelyPEB, na.rm = TRUE))
cs <- cbind(cs, BSCS_mean = rowMeans(BSCS, na.rm = TRUE))
cs <- cbind(cs, thermo_moral_mean = rowMeans(thermo_moral, na.rm = TRUE))
cs <- cbind(cs, MAC_mean = rowMeans(moral_ascoop, na.rm = TRUE))
cs <- cbind(cs, MFT_mean = rowMeans(moral_found, na.rm = TRUE))

#Rename column names to descriptives: cs %>% rename(new = old)
cs <- cs %>% rename('Political orientation' = Q35_1)
cs <- cs %>% rename('Habit' = Q8_2)
cs <- cs %>% rename('SocialNorm' = Q8_3)
cs <- cs %>% rename('PBC' = Q8_5)
cs <- cs %>% rename('AwarenessConsequences' = Q8_1)
cs <- cs %>% rename('Intention' = Q8_6)
cs <- cs %>% rename('AscriptionResponsibility' = Q8_4)

#Replace 0's with NA in Age
cs$Q34[cs$Q34 == 0] <- NA

#Centre Age
#cs$Q34 <- cs$Q34 - mean(cs$Q34, na.rm = TRUE)

#LOAD IN THE THERMOSTAT DATA

#Read in the Irus data (hourly, without interactions)
irus_data <- vroom("IrusData - Energy data Crescent and Warneford.csv",
  col_names = TRUE, col_select = c(1:5, 7, 10, 11, 25, 26))

#Filter for student room heaters only (takes out water heaters, kitchens, offices etc)
irus_data <- irus_data %>%
  filter(Type == "Room Heater", Name != "Office", nchar(Name) <= 6) %>%
  arrange(Name)

irus_data$Site <- as.factor(irus_data$Site)

#Load external temperature data (source: Weather Online)
external_temp <- tibble(fread("Oxford temp data.csv") %>%
  rename(Ext_temp_celsius = `Temp 2m [C]`) %>%
  select(Date, Ext_temp_celsius))
external_temp$Date <- as.Date(external_temp$Date, "%d/%m/%Y")

irus_data <- left_join(irus_data, external_temp, by = "Date")

#Merge the Site and Room Name fields
irus_data$site_room <- paste(irus_data$Site, irus_data$Name)

#Remove any spaces (to ensure matching)
irus_data$site_room <- gsub('\\s+', '', irus_data$site_room)

#Merge the date and time fields
irus_data$date_time <- fastPOSIXct(paste(irus_data$Date, irus_data$Time))

#Convert rooms to factors (for graphing)
irus_data$site_room <- as.factor(irus_data$site_room)

#Add daily mean setpoint per room and daily average per room into dataframe

irus_data <- irus_data %>% group_by(site_room, Date) %>% 
  mutate(daily_mean_sp = mean(Setpoint, na.rm = TRUE),
         daily_airtemp = mean(`Temp Air`, na.rm = TRUE))

irus_data <- left_join(irus_data, irus_data %>% 
                         group_by(site_room, Date) %>% 
                         filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
                         filter(Setpoint != 19) %>% mutate(daily_mean_sp_excdflt = mean(
                           Setpoint, na.rm = TRUE)))

irus_data <- left_join(irus_data, irus_data %>% 
                         group_by(site_room) %>% 
                         filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
                         filter(Setpoint != 19) %>% mutate(mean_sp_excdflt = mean(
                           Setpoint, na.rm = TRUE)))

#HYPOTHESIS 4 - add Before & After tags

irus_data$intervention <- NA
attach(irus_data)
irus_data$intervention[Date < as.Date("2020-02-14")] <- "Before"
irus_data$intervention[Date > as.Date("2020-02-14")] <- "After"
detach(irus_data)

#Add EXCLUDE filter for peaks
exclude <- irus_data %>% filter((Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) |
(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01"))) %>%
  group_by(date_time ,site_room ) %>%
  summarise(temp = Setpoint) %>%  summarise(temp = mean(temp, na.rm = TRUE)) %>% 
  arrange(desc(temp)) %>% filter (temp > 20) %>% pull(date_time)

# Add EXCLUDE flag to Irus dataframe - 0 = ones to ignore; 1 = ones to keep
irus_data <- irus_data %>% ungroup() %>% mutate(exclude = (ifelse(irus_data$date_time %in% exclude, 0, 1)))



#Calc mean room temp before and after posters (14th Feb) for each room & append to irus_data
irus_data <- left_join (irus_data, irus_data %>% 
  filter (Date > as.Date("2020-01-30") & Date < as.Date(  "2020-02-14")) %>% 
  group_by(site_room) %>% 
  summarise(avg_setpoint_before = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
   group_by(site_room) %>%
   summarise(avg_setpoint_after = mean(Setpoint)), by = "site_room")

#Calculate with cleaned data - exclude peaks
irus_data <- left_join (irus_data, irus_data %>% 
  filter (Date > as.Date("2020-01-30") & Date < as.Date(  "2020-02-14")) %>% filter (exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_setpoint_beforeCL = (mean(Setpoint) - 19)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% filter (exclude == 1) %>%
   group_by(site_room) %>%
   summarise(avg_setpoint_afterCL = (mean(Setpoint) - 19)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>%
 filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-03-01")) %>% filter (exclude == 1) %>%
   group_by(site_room) %>%
   summarise(avg_setpointCL = (mean(Setpoint) - 19)), by = "site_room")

 
#Calculate before and after average thermostat set points EXC. default settings + add to irus_data
irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdflt = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% 
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdflt = mean(Setpoint)), by = "site_room")


#Add EXC defaults, EXC peaks data to Irus
irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdfltCL = mean(Setpoint)), by = "site_room")

irus_data <- left_join (irus_data, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% 
  filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

#MEANS
irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)) %>% as.data.frame()

irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>% 
    filter(Setpoint != 19) %>% 
  filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>% filter(exclude == 1) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)) %>% as.data.frame()

irus_data <- irus_data %>% mutate (thermo_change_excdflt = avg_sp_after_excdflt - avg_sp_before_excdflt)

#Calculate number of settings below 19 degrees, before and after 14th Feb (posters date)

#Add thermostat data inc defaults to CS
cs <- left_join(cs, irus_data %>% 
    filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% 
    group_by(site_room) %>% 
    summarise(avg_setpoint_before = mean(Setpoint)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-02-14") & Date < as.Date(  "2020-03-01")) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpoint_after = mean(Setpoint)), by = "site_room")


cs <- left_join(cs, irus_data %>% 
    filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>% filter(exclude == 1) %>%
    group_by(site_room) %>% 
    summarise(avg_setpoint_beforeCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-02-14") & Date < as.Date(  "2020-03-01")) %>% filter(exclude == 1) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpoint_afterCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join(cs, irus_data %>% 
                  filter(Date > as.Date("2020-01-30") & Date < as.Date(  "2020-03-01")) %>% filter(exclude == 1) %>%
                  group_by(site_room) %>%
                  summarise(avg_setpointCL = (mean(Setpoint)-19)), by = "site_room")


#Add thermostat data exc defaults

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdflt = mean(Setpoint)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdflt = mean(Setpoint)), by = "site_room")

cs <- cs %>% mutate(avg_sp_before_excdflt19 = avg_sp_before_excdflt - 19,
                    avg_sp_after_excdflt19 = avg_sp_after_excdflt - 19)

#Add data exc dflt & exc peaks (cleaned)
cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-02-14")) %>%
  filter(exclude == 1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_before_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-02-14") & Date < as.Date("2020-03-01")) %>%
  filter(exclude ==1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_after_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

cs <- left_join (cs, irus_data %>% 
  filter(!(Setpoint == 21 & hour(date_time) >= 7 & hour(date_time) <= 10)) %>%
filter(Setpoint != 19) %>% filter(Date > as.Date("2020-01-30") & Date < as.Date("2020-03-01")) %>%
  filter(exclude ==1) %>%
  group_by(site_room) %>% 
  summarise(avg_sp_overall_excdfltCL = (mean(Setpoint)-19)), by = "site_room")

#Add change variable
cs <- cs %>% mutate(thermo_changeCL = avg_setpoint_afterCL - avg_setpoint_beforeCL)
cs <- cs %>% mutate (thermo_change_excdfltCL = avg_sp_after_excdfltCL - avg_sp_before_excdfltCL)

#Standardise key variables
cs <- cs %>% mutate(likelyPEB_mean_sz = as.numeric(scale(likelyPEB_mean)))
cs <- cs %>% mutate(EAI_mean_sz = as.numeric(scale(EAI_mean)))
cs <- cs %>% mutate(NEP_mean_sz = as.numeric(scale(NEP_mean)))
cs <- cs %>% mutate(PEB_activist_sz = as.numeric(scale(PEB_activist)))
cs <- cs %>% mutate(PEB_pragmatist_sz = as.numeric(scale(PEB_pragmatist)))
cs <- cs %>% mutate(thermo_moral_mean_sz = as.numeric(scale(thermo_moral_mean)))
cs <- cs %>% mutate(MAC_mean_sz = as.numeric(scale(MAC_mean)))
cs <- cs %>% mutate(MFT_mean_sz = as.numeric(scale(MFT_mean)))
# cs <- cs %>% mutate(sub19_before_sz = as.numeric(scale(sub19_before)))
# cs <- cs %>% mutate(sub19_after_sz = as.numeric(scale(sub19_after)))
cs <- cs %>% mutate(avg_setpoint_before_sz = as.numeric(scale(avg_setpoint_before)))
cs <- cs %>% mutate(avg_setpoint_after_sz = as.numeric(scale(avg_setpoint_after)))
cs <- cs %>% mutate(avg_sp_before_excdflt_sz = as.numeric(scale(avg_sp_before_excdflt)))
cs <- cs %>% mutate(avg_sp_after_excdflt_sz = as.numeric(scale(avg_sp_after_excdflt)))
cs <- cs %>% mutate(thermo_changeCL_sz = as.numeric(scale(thermo_changeCL)))
# cs <- cs %>% mutate(sub19_change_sz = as.numeric(scale(sub19_change)))
cs <- cs %>% mutate(thermo_change_excdfltCL_sz = as.numeric(scale(thermo_change_excdfltCL)))
cs <- cs %>% mutate(AwarenessConsequences_sz = as.numeric(scale(AwarenessConsequences)))
cs <- cs %>% mutate(Habit_sz = as.numeric(scale(Habit)))
cs <- cs %>% mutate(Intention_sz = as.numeric(scale(Intention)))
cs <- cs %>% mutate(SocialNorm_sz = as.numeric(scale(SocialNorm)))
cs <- cs %>% mutate(AscriptionResponsibility_sz = as.numeric(scale(AscriptionResponsibility)))
cs <- cs %>% mutate(PBC_sz = as.numeric(scale(PBC)))
cs <- cs %>% mutate(BSCS_mean_sz = as.numeric(scale(BSCS_mean)))
cs <- cs %>% mutate(PoliticalOrientation_sz = as.numeric(scale(`Political orientation`)))




```

## EXPORT FILES FOR QUICK ACCESS (if required)
```{r FileExport, eval = FALSE, echo=FALSE}

# main data file for longitudinal modelling (LARGE; 700k + rows)
write_csv(irus_data, file = "IrusData.csv")

# data file for survey data (Small; 88 rows)
write_csv(cs, file = "SurveyData.csv")
```

## IMPORT dissertation datafframe AND CREATE new DATA-FRAME (df) for modelling
```{r DataImport, echo=FALSE, results = FALSE, eval=FALSE}

#Import csv's
irus_data <- read.csv("IrusData.csv")
cs <- read.csv("SurveyData.csv")

#Convert date_time variable to correct to POSIXt format
irus_data$date_time <- as_datetime(irus_data$date_time)

#Create new data-frame for temperature modelling with appropriate variables
df <- irus_data %>%
  filter(Date >= as.Date("2020-02-01")) %>%  #dropping all data points before Feb 1
  filter(Date <= as.Date("2020-02-29")) %>%
  filter(Date != as.Date("2020-02-14")) %>% #remove 14th Feb as intervention time unknown
  filter(is.na(Setpoint) == 0) %>%
  #filter(Setpoint != 19) %>% #exclude all default values
  mutate(time_elapsed = as.numeric((date_time) - date_time[1])/3600) %>% #zero time clock and convert to hours
  mutate(temp.z = (Setpoint - mean(Setpoint,  na.rm=T))/sd(Setpoint, na.rm=T)) %>% #standardise set-point temperature
  mutate(temp.c = Setpoint - 19) %>% #create centred (non-standardised) temp setpoint measure = difference from default
  mutate(external.z = (Ext_temp_celsius - mean(Ext_temp_celsius,  na.rm=T))/sd(Ext_temp_celsius, na.rm=T)) #standardise external temperature

#Replace all unique room names (characters) with number for modelling
cross_table <- data.frame(Name = unique(df$Name),Code = seq(1:length(unique(df$Name))))
df$Address <- as.integer(lapply(df$Name, function(x) cross_table$Code[match(x, cross_table$Name)]))

# Convert intervention variable into factor
df$intervention <- factor(df$intervention, levels = c("Before", "After"))

#Take sample of data to test models (faster?)
#df_sample <- df[sample(nrow(df), 50000), ]

#Dataframe excluding temp = 19 default values
df_xd <- df %>% filter(Setpoint != 19)

#rescale time_elapsed variable - that works!
df$time_elapsed_r <- df$time_elapsed/300
df_xd$time_elapsed_r <- df_xd$time_elapsed/300

#create moralisation score
#create new variable = moralisation from Q8_7 and Q8_9
cs <- cs %>% rowwise() %>% mutate(moralisation = mean(c(Q8_7, Q8_9)))
#standardise
cs$moralisation.z <- (cs$moralisation - mean(cs$moralisation))/sd(cs$moralisation)

#Count instances of room numbers - there is one duplicate in Crescent - Room L04F
data.frame(table(cs$site_room))

#remove duplicate room - CRESCENTL04F
cs <- cs %>% filter(site_room != "CRESCENTL04F")

#Create survey data data-frame by matching all completed surveys (cs) variables to main df only for those that have completed

df_survey <- inner_join(df, cs[,c("site_room", "Habit_sz", "PBC_sz", "moralisation.z",
                                   "AscriptionResponsibility_sz", "SocialNorm_sz",
                                   "AwarenessConsequences_sz", "PEB_pragmatist_sz",
                                   "PEB_activist_sz", "NEP_mean_sz", "Intention_sz",
                                  "Political.orientation", 'Q34', 'Q17', 'Q13')],
                        by = "site_room")
df_survey_xd <- inner_join(df_xd, cs[,c("site_room", "Habit_sz", "PBC_sz", "moralisation.z",
                                   "AscriptionResponsibility_sz", "SocialNorm_sz",
                                   "AwarenessConsequences_sz", "PEB_pragmatist_sz",
                                   "PEB_activist_sz", "NEP_mean_sz", "Intention_sz",
                                  "Political.orientation", 'Q34', 'Q17', 'Q13')],
                           by = "site_room")

df_survey <- rename(df_survey, 'Age' = 'Q34', 'Born_UK' = 'Q17', 'Gender' = 'Q13')


#Rename site locations with communications condition
df$Site <- recode_factor(df$Site, CRESCENT = "FAMILY", WARNEFORD = "DIRECT")

#generate sample of 50 rooms for smaller dataframe for model testing

df_sample <- filter(df, Address %in%  sample(1:551, 50))

#save as rds objects for easy recovery
saveRDS(df_survey, "df_survey.rds")
saveRDS(df_survey_xd, "df_survey_xd.rds")
saveRDS(df, "df.rds")
saveRDS(df_xd, "df_xd.rds")
saveRDS(cs, "cs.rds")
saveRDS(df_sample, "df_sample.rds")


```

##New dataframe load in RDS objects
```{r}

df_survey <- loadRDS("df_survey.rds")
df_survey_xd <- loadRDS("df_survey_xd.rds")
df <- loadRDS("df.rds")
df_xd <- loadRDS("df_xd.rds")
cs <- loadRDS("cs.rds")
df_sample <- loadRDS("df_sample.rds")

```

##Linear models and visualisation
```{r LinearModels, eval=FALSE, echo=FALSE}

#Modelling manipulation effect - linear models
lm1 <- lm(temp.c ~ intervention, data = df)
summary(lm1)
lm2 <- lm(temp.c ~ time_elapsed_r + intervention, data = df)
summary(lm2)
lm3 <- lm(temp.c ~ time_elapsed_r + intervention + external.z, data = df)
summary(lm3)


# histogram of temp distributions - full data frame and excluding defaults 

options(scipen = 6) # sets threshold for number of characters to 6 for scientific notation

h1 <- df %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13)
h2 <- df_xd %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13)
h3 <- df %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13) + facet_wrap(df$intervention) + ggtitle("Data including default settings (19 celsius)") + theme(plot.title = element_text(size = 10))
h4 <- df_xd %>% ggplot() + geom_histogram(aes(x = temp.c), binwidth = .5) + xlim(-5,13) + facet_wrap(df_xd$intervention)+ ggtitle("Data excluding default settings") + theme(plot.title = element_text(size = 10))

gridExtra::grid.arrange(h3, h4)

df %>%
  ggplot( aes(x=temp.c, fill=intervention), binwidth = .5) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'dodge') +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")



h5 <- df_xd %>% ggplot(aes(x=temp.c, fill=intervention)) + geom_histogram(binwidth = .5, color="#e9ecef", alpha=0.6, position = 'dodge') + xlim(-5,5) + facet_wrap(~ Site) + ggtitle("Data excluding default settings") +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum(plot_title_size = 9, plot_title_face = "plain", strip_text_size = 8)
h5


```

## Mixed models
```{r MixedModels}



# WITH rescaled time axis, the models converge..

#Including defaults
mm1 <- lmer(temp.c ~ (time_elapsed_r|Address), data = df)
summary(mm1)

#Excluding defaults
mmxd1 <- lmer(temp.c ~ (time_elapsed_r|Address), data = df_xd)
summary(mmxd1)

# No significant effect of time as a fixed effect (t value < 1)
mm2 <- lmer(temp.c ~ time_elapsed_r + (time_elapsed_r|Address), data = df)
summary(mm2)

# Controlling for external temp
# Significant effect of external temp. - slightly greater effect of time 
mm3 <- lmer(temp.c ~ time_elapsed_r + Ext_temp_celsius + (time_elapsed_r|Address), data = df)
summary(mm3)

#Then ext temp and intervention together
# Shows intervention has an effect too
mm5 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address), data = df)
summary(mm5)

# equivalent model excluding defaults
mmx5 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address), data = df_xd)
summary(mmx5)

#Check for interaction effect of Site*Intervention
#Shows that the intervention effect is driven largely (wholly??) by WARNEFORD site
# Note: from memory we found this before - there was a big effect in one hall and not the other
# personally I doubt this was solely due to a few words different on the poster, but that's for discussion
mm6 <- lmer(temp.c ~ time_elapsed_r + intervention*Site + Ext_temp_celsius + (time_elapsed_r|Address), data = df)
summary(mm6)

mmx6 <- lmer(temp.c ~ time_elapsed_r + intervention*Site + Ext_temp_celsius + (time_elapsed_r|Address), data = df_xd)
summary(mmx6)

tab_model(mmx5, mmx6)


#Random effects of time, within room and site implicitly nested 
mm7 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address) + (time_elapsed_r|Site), data = df)
summary(mm7)

# try with bobyqa - fails to converge (sometimes)
# and doesn't really tell us anything new anyway
# mm7b <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r|Address) + (time_elapsed_r|Site), data = df, control = lmerControl(optimizer = "bobyqa"))
# summary(mm7b)

#Random effects of time and intervention implicitly nested within Site and Room
#boundary (singular) fit: see ?isSingular - too complex?
#mm8 <- lmer(temp.c ~ time_elapsed_r + intervention + Ext_temp_celsius + (time_elapsed_r+intervention|Address) + (time_elapsed_r+intervention|Site), data = df, control = lmerControl(optimizer = "bobyqa"))
#summary(mm8)






```


## Ordinal models for multi-nomial (categorical) DV
```{r OrdinalModels, eval=FALSE}

#Add outcome variable to dataframe
df$behaviour <- NA
df$behaviour[df$Setpoint == 19] = "default"
df$behaviour[df$Setpoint < 19] = "lower"
df$behaviour[df$Setpoint > 19] = "higher"
df$behaviour <- factor(df$behaviour, ordered = TRUE, levels = c("lower", "default", "higher"))


# Ordinal dependent variable models from https://rpubs.com/rslbliss/r_logistic_ws

# Model 1
model_OD1 <- polr(behaviour ~ time_elapsed_r + intervention*Site + Ext_temp_celsius, data = df, Hess=TRUE)
summary(model_OD1)

## calculate and store p values
ctable <- coef(summary(model_OD1))
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
## combined table
(ctable <- cbind(ctable, "p value" = p))

## odds ratios and confidence intervals
ci <- confint.default(model_OD1)
exp(cbind("Odds ratio" = coef(model_OD1), ci))

#convert Address to factor
df$Address <- factor(df$Address)
#same for Site
df$Site <- factor(df$Site)

## Attempt to fit Ordinal model with random effects - this attempt didn't work

#Fit random effect grouping under participant using clmm2 from ordinal package
# model_OD2 <- ordinal::clmm2(behaviour ~ time_elapsed_r + intervention*Site + Ext_temp_celsius,
#                           random = Address, data = df, Hess=TRUE, nAGQ = 1)
# summary(model_OD2)

# Warning: clmm2 may not have converged:
#   optimizer 'ucminf' terminated with max|gradient|: 60.803516159627

#saveRDS(model_OD2, file = "model_OD2.rds")
model_OD2 <- readRDS(file = "model_OD2.rds")
summary(model_OD2)

# this doesn't converge either
# Warning: clmm2 may not have converged:
#   optimizer 'ucminf' terminated with max|gradient|: 0.395293339186445
# model_OD3 <- clmm2(behaviour ~ time_elapsed_r + intervention + Ext_temp_celsius,
#                           random = Address, data = df, Hess=TRUE, nAGQ = 1)
# summary(model_OD3)



## Attempt 2 using GLMMadaptive package

#need to create "cohort" variable (see https://drizopoulos.github.io/GLMMadaptive/articles/Ordinal_Mixed_Models.html)

cr_vals <- cr_setup(df$behaviour)
cr_data <- df[cr_vals$subs, ]
cr_data$y_new <- cr_vals$y
cr_data$cohort <- cr_vals$cohort

# This runs and converges, but only gives coefficients for behaviour >= default (ie. higher+default versus lower), which is not what we want - the example in the documentation does the same thing.. je ne comprends pas!
m_OD1 <- mixed_model(y_new ~ cohort + intervention + time_elapsed_r, random = ~ time_elapsed_r | site_room, data = cr_data, family = binomial())
saveRDS(m_OD1, file = "m_OD1.rds")
summary(m_OD1)

#Attempt multi-nomial mixed effect model - documentation for this is sparse and so I'm not even sure the syntax is right, but it crashes R every time..

# mixcat::npmlt(df$behaviour ~ df$time_elapsed_r + df$intervention*df$Site + df$Ext_temp_celsius, random=~1+df$time_elapsed_r, id = df$Address)

```

## Survey data
```{r SurveyDataMixedModels}


####Manipulation effect in sub-sample with questionnaire data
#NO EXCLUSIONS
m1 <- lmer(temp.c ~ 1 + time_elapsed_r + (time_elapsed_r|site_room), data=df_survey)
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m3 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m4 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))

m5 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
tab_model(m1, m2, m3, m4, m5)

#EXCLUSIONS 
m1 <- lmer(temp.c ~ 1 + time_elapsed_r + (time_elapsed_r|site_room), data=df_survey_xd)
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m3 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m4 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention + Site + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
m5 <- lmer(temp.c ~ 1 + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey_xd, control = lmerControl(optimizer = "bobyqa"))
tab_model(m1, m2, m3, m4, m5)



#these variables seem to have no effect 

m1 <- lmer(temp.c ~ 1 + NEP_mean_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + PEB_activist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + PEB_pragmatist_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + Habit_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + SocialNorm_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

#These variables are all p < .05 - strongest are intention and moralisation
m1 <- lmer(temp.c ~ 1 + AwarenessConsequences_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + AscriptionResponsibility_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + PBC_sz + time_elapsed_r +external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + moralisation.z + time_elapsed_r +external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + Intention_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)
m1 <- lmer(temp.c ~ 1 + NEP_mean_sz + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room),data=df_survey)
summary(m1)

#Demographic variables
#political
m5 <- lmer(temp.c ~ 1 + Political.orientation + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)

#gender
#ignore single non-binary participant
m5 <- lmer(temp.c ~ 1 + Gender + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=(df_survey %>% filter(Gender != 'Different Identity')), control = lmerControl(optimizer = "bobyqa"))
summary(m5)

#UK - SIGNIFICANT
m5 <- lmer(temp.c ~ 1 + Born_UK + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)

#Age - convert to age from 18 = 0
df_survey$Age <- df_survey$Age - min(df_survey$Age)
m5 <- lmer(temp.c ~ 1 + Age + time_elapsed_r + external.z + intervention * Site + (time_elapsed_r|site_room), data=df_survey, control = lmerControl(optimizer = "bobyqa"))
summary(m5)


#In multi-variate models only INTENTION is significant
m2 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + Intention_sz + (time_elapsed_r|site_room),data=df_survey)
summary(m2)
tab_model(m2)

# Excluding Intention, Moralisation is still not significant
m3 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + (time_elapsed_r|site_room),data=df_survey)
summary(m3)
tab_model(m3)

# Same result on excl. default data
m4 <- lmer(temp.c ~ 1 + time_elapsed_r + AwarenessConsequences_sz + AscriptionResponsibility_sz +
             PBC_sz + moralisation.z + (time_elapsed_r|site_room),data=df_survey_xd)
summary(m4)
tab_model(m4)

```


## Bayesian Models
```{r BayesModels, eval = FALSE}


# Fit models using sample data-frame
bms1 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address), 
            data = df_sample, file=("bms1"), file_refit = getOption("brms.file_refit", "on_change"),
            cores = 4, iter = 4000,
            prior = set_prior("normal(0,0.5)", class = "b"))

summary(bms1)

#Fit model using full data-set
bm1 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address), 
            data = df, file=("bm1"), file_refit = getOption("brms.file_refit", "on_change"),
            cores = 4, iter = 5000,
            prior = set_prior("normal(0,0.4)", class = "b"))

summary(bm1)

#need to add cores = 4 as default is 1

# NULL model

# Initial null model - intercept only - does converge
bm1 <- brm(formula = temp.c ~ (time_elapsed|Address), data = df,
           warmup = 1000, iter = 3000, cores = 4, control = list(adapt_delta = 0.95),
           file=("bm1"), file_refit = getOption("brms.file_refit", "on_change"))

#NULL model with transposed time - NOT yet FIT
bm1r <- brm(formula = temp.c ~ time_elapsed_r + (time_elapsed_r|Address), data = df,
           file=("bm1r"), file_refit = getOption("brms.file_refit", "on_change"))

#temp depends on intervention - model COMPLETE
bm2 <- brm(formula = temp.c ~ intervention,
          data = df, cores = 4,
          file=("bm2"), control = list(adapt_delta = 0.95),
          file_refit = getOption("brms.file_refit", "on_change"))

#temp depends on intervention - model DOES NOT CONVERGE (YET)
bm3 <- brm(formula = temp.c ~ time_elapsed_r + intervention + (time_elapsed|Address),
          data = df, cores = 4, warmup = 1500, iter = 4000,
          file=("bm3"), file_refit = getOption("brms.file_refit", "on_change"))

n_divergent <- function(x) {
    stopifnot(is(x, "brmsfit"))
    out <- lapply(x$fit@sim$samples, function(y) 
        sum(attr(y, "sampler_params")[["divergent__"]]))
    sum(unlist(out))
}

n_divergent(bm1)
n_divergent(bm2)
n_divergent(bm3)

#model visualisation and diagnostics
posterior1 <- as.array(bms1)

# Trace plot of Linear Regression, define variables to plot via pars flag
mcmc_trace(posterior1,  pars = c("b_Intercept", "sigma"), 
           n_warmup = 1000, facet_args = list(ncol = 2, labeller = label_parsed))

mcmc_acf(posterior1, pars = c("b_Intercept", "sigma"), 
         lags = 15)

```


